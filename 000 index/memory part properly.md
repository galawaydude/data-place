This is an excellent set of notes, covering many fundamental concepts in Operating Systems, particularly Memory Management. I will go through each section of your handwritten notes, converting them into structured Markdown, and providing a detailed, in-depth explanation with examples to ensure you grasp every concept thoroughly.

Let's begin!

---

# Memory Management: An In-Depth Exploration

Memory Management is a crucial function of an operating system, responsible for managing the computer's primary memory (RAM). Its main goal is to allocate and deallocate memory space for processes, ensuring efficient utilization of memory resources, protection between processes, and enabling a high degree of multiprogramming.

## 1. The Need for Multiprogramming and CPU Utilization

Multiprogramming is the ability of an operating system to execute multiple processes concurrently by rapidly switching the CPU between them. This is essential for modern computing to maximize CPU utilization and system throughput.

### 1.1 Components of a Computer System

The CPU interacts with various memory components in a hierarchy:

*   **Registers (Regs):** Smallest, fastest, and most expensive memory, located inside the CPU. Used to hold data and instructions currently being processed.
*   **Cache:** A small, fast memory that stores copies of data from frequently used main memory locations. It's faster than main memory but slower than registers.
*   **Main Memory (RAM - Random Access Memory):** The primary storage where programs and data are loaded for execution. Much larger than cache but slower.
*   **Hard Disk Drive (HDD) / Solid State Drive (SSD):** Secondary storage, much larger and cheaper than RAM, but significantly slower. Used for persistent storage of programs and data.

**Problem:** All these components (especially Registers, Cache, and RAM) are **costly**. Given the cost, we want to maximize their utility. This is where multiprogramming comes in.

### 1.2 CPU Utilization in Multiprogramming

A key metric in multiprogramming is **CPU Utilization**, which measures the percentage of time the CPU is actively executing instructions, rather than waiting (e.g., for I/O operations).

**Let's define `p`:**
`p` = The probability that a process is waiting for I/O (Input/Output) to complete.
*   **Explanation:** Processes don't just compute; they also frequently perform I/O operations (reading from disk, writing to a screen, network communication, user input). During an I/O operation, the CPU sits idle for that process. If `p` is the probability of a process waiting for I/O, then `(1-p)` is the probability that the process is actively using the CPU (performing a CPU burst).

**Consider a system with `n` processes in memory (degree of multiprogramming).**
For the CPU to be idle, *all* `n` processes must *simultaneously* be waiting for I/O.
The probability of one process waiting for I/O is `p`.
The probability of `n` processes *all* waiting for I/O (assuming independence) is `p * p * ... (n times) = p^n`.

Therefore, the probability that at least one process is *not* waiting for I/O (i.e., at least one process is ready to use the CPU) is `1 - p^n`.

**CPU Utilization = 1 - p^n**

Let's walk through your examples:

**Example 1:**
*   **Scenario:** 4 MB Main Memory (MM), 4 MB process.
*   **Degree of Multiprogramming (n):** Since one process needs 4 MB and we only have 4 MB, only `n=1` process can be in memory at a time.
*   **CPU Utilization:** `(1 - p^1) = (1 - p)`
*   **If p = 80% (0.8):**
    *   CPU Utilization = `1 - 0.8 = 0.2` or **20%**.
    *   **Explanation:** This is very low! It means 80% of the time, the CPU is idle because the single process in memory is waiting for I/O. This is inefficient.

**Example 2:**
*   **Scenario:** 16 MB MM, 4 MB process.
*   **Number of processes (n):** `16 MB / 4 MB = 4 processes` can be in memory.
*   **CPU Utilization:** `(1 - p^4)`
*   **If p = 80% (0.8):**
    *   CPU Utilization = `1 - (0.8)^4 = 1 - 0.4096 = 0.5904` or **~59%**.
    *   **Explanation:** By allowing 4 processes to reside in memory, even if one is waiting for I/O, there's a good chance another is ready to run. This significantly increases CPU utilization.

**Example 3:**
*   **Scenario:** 32 MB MM, 4 MB process.
*   **Number of processes (n):** `32 MB / 4 MB = 8 processes`
*   **CPU Utilization:** `(1 - p^8)`
*   **If p = 80% (0.8):**
    *   CPU Utilization = `1 - (0.8)^8 = 1 - 0.16777 = 0.83223` or **~83%**.
    *   **Explanation:** Further increasing `n` brings utilization closer to 100%.

**Example 4:**
*   **Scenario:** 48 MB MM, 4 MB process.
*   **Number of processes (n):** `48 MB / 4 MB = 12 processes`
*   **CPU Utilization:** `(1 - p^12)`
*   **If p = 80% (0.8):**
    *   CPU Utilization = `1 - (0.8)^12 = 1 - 0.0687 = 0.9313` or **~93%**.
    *   **Explanation:** The trend is clear: as `n` increases, CPU utilization approaches 100%.

**General Formula:** **CPU Utilization = 1 - p^n**
Where `n` is the number of processes that can be present in memory at the same time.

**Key takeaway:** CPU utilization is directly proportional to the degree of multiprogramming (`n`).
*   As `n -> âˆž` (n approaches infinity), CPU utilization `-> 1` (100%).
*   **However, for CPU utilization to be 100%, `n` would theoretically need to be infinite.** This is not practical.
*   For this, Main Memory size should be very, very costly.
*   **The solution:** We need to **use the memory in such a way that we can achieve a high degree of multiprogramming without acquiring an infinite (or prohibitively expensive) amount of physical memory.** This is the fundamental challenge memory management addresses.

## 2. Object Code Relocation & Linkers

Before a program can run, it undergoes several transformations. One critical phase involves handling addresses, particularly when combining different parts of a program or linking with external libraries.

### 2.1 Program Compilation and Assembly Flow

The journey from source code to an executable program typically involves these steps:

1.  **Source Code:** Human-readable code (e.g., C++, Java).
2.  **Compiler:** Translates source code into **assembly code**. It performs lexical analysis, syntax analysis, semantic analysis, intermediate code generation, code optimization, and target code generation.
3.  **Assembly Code:** A low-level programming language that is a symbolic representation of machine code.
4.  **Assembler:** Translates assembly code into **object code (machine code)**. It also builds initial symbol tables and resolves symbolic addresses within the current module.
5.  **Object Code (Object Modules):** Machine-readable code containing instructions and data, but typically with "placeholder" addresses for external references. These are often referred to as relocatable object files.
6.  **Linker:** Combines multiple object modules (and libraries) into a single **executable file**. It resolves external symbol references and performs relocation.
7.  **Loader:** Loads the executable file into main memory, assigning actual physical addresses, so the CPU can execute it.

### 2.2 Structure of an Object Code Module

An object code file (or object module) is not just raw machine instructions. It's structured with various sections to facilitate linking and loading:

*   **Header:** Contains basic information about the module, such as its size, creation date, and entry points.
*   **Text Segment (Code Segment):** Contains the actual machine instructions (the executable code) for the module.
*   **Data Segment:** Contains initialized global and static variables.
*   **Relocation Info (Relocation Table):** This is crucial. It lists all the instructions and data locations within the module that depend on absolute memory addresses. These addresses need to be adjusted (relocated) when the module is loaded into memory.
    *   **Absolute Address:** A fixed, precise memory location (e.g., `0x12345678`).
    *   **Relocatable Address:** An address specified relative to the start of the module or another base address. This address will change when the module is loaded.
    *   **Relocation:** The process of converting relocatable addresses into absolute addresses once the program's load-time memory location is known.
*   **Symbol Table:** Contains information about the symbols (functions and global variables) defined *within* this object module and symbols that are *referenced externally* by this module but defined elsewhere.
    *   Example: `min()`, `printf()`, `scanf()` functions with their addresses (which might be placeholders initially, or offsets).
    *   **Resolution:** The process of finding the actual memory addresses for these external symbols. This is done by the **linker**.
*   **Debugging Info:** Optional information used by debuggers, such as mappings from machine code back to source code lines, variable names, etc.

### 2.3 The Linker's Role

The linker is a system program that takes one or more object modules generated by the assembler/compiler and combines them into a single executable file.

**Why is it needed?**
*   **Modular Programming:** Programs are often split into multiple source files (modules) for better organization and reusability. Each module is compiled/assembled separately into an object file. The linker brings them together.
*   **Library Usage:** Programs often use functions from standard libraries (e.g., `printf` from C standard library). The linker includes the necessary library code or resolves references to it.

**Linker Process (Typically Two Passes):**

**Phase I (Pass 1):**
1.  **Build Segment Table:** The linker scans all input object modules (A, B, C, D, E, etc., and system libraries). For each module, it determines the size of its code and data segments.
2.  **Build Global Symbol Table:** It collects all *definitions* of global symbols (functions and global variables) from all input object modules and their *relative addresses*. It notes down which module defines which symbol and its offset within that module.
    *   Example: If `object code A` calls `B, C, D, E`, and `object code B` calls `P, Q`. The linker will find the actual locations of `B, C, D, E` and `P, Q` (if defined in other modules or libraries) and record them.

**Phase II (Pass 2):**
1.  **Generate Executable Code:** The linker now combines the code and data segments from all object modules into a single, contiguous block of memory within the executable file.
2.  **Resolve Undefined References:** Using the global symbol table built in Phase I, the linker goes back through the code. For every external call (like `call printf()`) or reference to an external variable, it fills in the correct **relative address** (or a placeholder that the loader will later convert to an absolute address).
3.  **Perform Relocation:** Based on the relocation information in each object module, the linker adjusts all addresses that were relative to the start of their original modules, making them relative to the start of the *combined executable file*.

**Linker Responsibilities:**

1.  **Relocation:** Adjusting relative addresses within object modules to reflect their positions within the final combined executable file.
2.  **Symbol Resolution:** Finding the definitions for all external symbols referenced by modules and linking them correctly. This includes linking with library routines.
    *   **Stub (or Glue Code):** For dynamically linked libraries (DLLs/shared libraries), the linker might insert a small piece of code called a "stub" or "glue code." Instead of directly embedding the library function's code, this stub is a placeholder that, at runtime, will resolve to the actual library function's address if it's not already loaded. For example, for a `printf()` call, a stub might point to the dynamic linker which then finds the `printf` function in a shared library.

### 2.4 The Loader

The loader is the final system software component involved in preparing a program for execution.

*   **Role:** The loader takes the **linked object code (the executable file)**, which is typically "relocatable" (meaning its addresses are relative to its own start), and places it into the main memory at an available starting address.
*   **Responsibilities:**
    1.  **Program Loading:** Loading the program's text and data segments from the executable file on disk into main memory.
    2.  **Relocation:** Performing the final adjustment of addresses from being relative to the executable file's start to being absolute physical addresses within RAM. This is done by adding the program's actual load address (base address) to all relative addresses.
    3.  **Symbol Resolution (if needed):** For dynamically linked libraries, the loader (or a dynamic linker invoked by the loader) will perform symbol resolution at run time.

**Relationship between Linker and Loader:**

*   **Linker:** Performs symbol resolution and relocation *to create a single executable file from multiple object files*. The output is typically **relocatable code**.
*   **Loader:** Performs program loading and *final relocation to absolute physical addresses* when placing the executable into RAM. For dynamic linking, it might also do run-time symbol resolution.

### 2.5 Different "Times" in Program Execution

The process of fixing addresses and resolving symbols can happen at different stages, influencing flexibility and performance:

1.  **Compile Time:**
    *   **Concept:** If the program's exact physical memory location is known *before compilation*, the compiler can generate absolute addresses directly.
    *   **Problem:** This is extremely inflexible. The program can only run at that specific address. If multiple programs need to run or if the memory layout changes, it won't work. This is rarely used in modern general-purpose OS.
    *   **Example from notes:** "Symbol constants -> relocatable address." This is a bit misleading here. It usually implies that if fixed, it's fixed *before* linking/loading.

2.  **Link Time:**
    *   **Concept:** The linker generates a relocatable executable file, but the addresses within this file are *relative* to the start of the executable. If the program is always loaded at the *same* starting address in memory, the linker can fix all addresses to be absolute.
    *   **Use case:** Simple embedded systems or very specific scenarios where the load address is fixed.
    *   **Example from notes:** "Relocatable code -> relocatable addresses ext. obj." Meaning, external object references are resolved to be relocatable within the final combined executable.

3.  **Load Time:**
    *   **Concept:** The linker produces a relocatable executable. The loader, when placing the program into memory, gets to know the program's *actual starting physical address*. It then performs the final **relocation** by adding this starting address to all the program's internal (relative) addresses.
    *   **Advantage:** Program can be loaded anywhere in memory. This is common for older systems or simpler executables.
    *   **Example from notes:** "Relocatable address -> absolute addresses."

4.  **Run Time (Dynamic Linking):**
    *   **Concept:** Relocation and symbol resolution can be postponed until the program is already running. This is typically done for **Dynamic Link Libraries (DLLs in Windows, Shared Objects/Libraries in Linux)**.
    *   **How it works:** When a program needs a function from a dynamic library, the linker doesn't embed the library's code. Instead, it places a stub. When the function is called for the first time, an OS component (the dynamic linker/loader) intervenes, finds the library in memory (or loads it if not present), and resolves the actual address of the function. Subsequent calls directly use the resolved address.
    *   **Advantages:**
        *   **Memory Saving:** Multiple programs can share a single copy of a library in memory.
        *   **Flexibility:** Library updates don't require recompiling or relinking all applications that use them.
        *   **Smaller Executables:** The executable file size is smaller as library code is not embedded.
    *   **Disadvantage:** Runtime overhead for resolving addresses.
    *   **Requirement:** Requires **OS Support** (e.g., dynamic linker).

### 2.6 Gate Problems (Illustrative Questions)

**Gate Q35: What information need not be included in an object module?**
*   a) Object code
*   b) Relocation bits
*   c) Names & locations of all external symbols defined in the object module.
*   d) Absolute addresses of internal symbols.

*   **Explanation:**
    *   a) Object code (machine instructions) is the core of an object module. Necessary.
    *   b) Relocation bits/info are essential for the linker/loader to adjust addresses. Necessary.
    *   c) The symbol table *must* list defined external symbols (for others to link to) and referenced external symbols (for the linker to resolve). Necessary.
    *   d) **Absolute addresses of internal symbols:** Object modules are *relocatable*. They contain addresses relative to their own start, not absolute physical addresses. The final absolute addresses are determined by the loader at load time. So, this information is not needed *in the object module itself*.
*   **Answer: (d)**

**Gate Q36: In a resident OS computer, which of the following system S/W must reside in the main memory under all situations?**
*   a) Assembler
*   b) Linker
*   c) Loader
*   d) Compiler

*   **Explanation:**
    *   Assembler, Linker, Compiler are tools used for program development. They are loaded into memory when needed (e.g., when you compile a program) and then swapped out. They don't need to be *resident* (always in memory).
    *   The **Loader** is responsible for bringing programs into memory for execution. It's a fundamental part of the OS that must always be available to load any process or even parts of the OS itself. In a "resident OS" context, this implies the core OS components are always in RAM.
*   **Answer: (c)**

**Gate Q01: The process of assigning load addresses to the various parts of the program and adjusting the code and data in the program to reflect the assigned addresses is called...**
*   a) Assembly
*   b) Parsing
*   c) Relocation
*   d) Symbol resolution

*   **Explanation:**
    *   a) Assembly: Translating assembly code to object code.
    *   b) Parsing: Part of compilation (syntax analysis).
    *   c) **Relocation:** This precisely describes the adjustment of addresses based on the program's load address. It's the final step of address binding.
    *   d) Symbol resolution: Finding definitions of external symbols.
*   **Answer: (c)**

## 3. Memory Partitioning

Memory partitioning is a fundamental technique in memory management for dividing main memory into sections to accommodate multiple processes.

### 3.1 Contiguous Memory Allocation

*   **Concept:** In contiguous memory allocation, each process is allocated a single, contiguous block of memory. This means all parts of the program (code, data, stack, heap) must reside together in one unbroken chunk.

### 3.2 Fixed Partitioning (Static Partitioning)

*   **Concept:** Main memory is divided into a fixed number of partitions *before* program execution. These partitions can be of equal or unequal sizes. Once defined, their sizes do not change.
*   **Diagram:** Imagine RAM divided into pre-defined slots: OS, 2MB, 4MB, 4MB, 16MB.

*   **Major Problems with Fixed Partitioning:**

    1.  **Internal Fragmentation:**
        *   **Explanation:** This occurs when a process is allocated a memory block larger than it actually needs. The unused space *within* the allocated partition is called internal fragmentation.
        *   **Example:** If a process needs 3MB and is placed in a 4MB partition, 1MB (4MB - 3MB) is wasted *inside* that partition. This space cannot be used by any other process, even if free.
        *   **Consequence:** Wastes memory.

    2.  **Limitation on Process Size:**
        *   **Explanation:** A process can only be loaded if its size is less than or equal to the size of the *largest available partition*. If a process is larger than any single partition, it cannot be loaded, even if the total free memory is sufficient.
        *   **Consequence:** Limits the types and sizes of programs that can run.

    3.  **External Fragmentation:**
        *   **Explanation:** This occurs when there is enough total free memory to satisfy a request, but the free memory is scattered in small, non-contiguous blocks across different partitions, making it impossible to allocate a single large contiguous block to a process.
        *   **Example:** If you have a 2MB free partition and a 3MB free partition, and a 4MB process arrives, it cannot be loaded, even though 5MB (2+3) of total memory is free.
        *   **Consequence:** Reduces the degree of multiprogramming and efficient memory utilization.

    4.  **Limited Degree of Multiprogramming:**
        *   **Explanation:** The number of processes that can reside in memory at any given time is limited by the fixed number of partitions. Even if some partitions are empty, new processes can only be loaded if there's an available partition of suitable size.
        *   **Consequence:** Underutilization of CPU if `n` is too small.

### 3.3 Relocation & Protection in Hardware (Memory Management Unit - MMU)

To overcome the rigidities of fixed memory allocation and ensure process isolation, hardware support is crucial. This is provided by the **Memory Management Unit (MMU)**, which is a hardware device.

*   **Logical Address (Virtual Address):** The address generated by the CPU. Processes generate logical addresses.
*   **Physical Address (Real Address):** The actual address in main memory. The MMU translates logical addresses into physical addresses.

**How MMU works with Relocation and Limit Registers:**

The MMU uses two special registers for address translation and protection in simple contiguous allocation schemes:

1.  **Relocation Register (Base Register):**
    *   **Purpose:** Stores the starting physical address (base address) where the current process is loaded in main memory.
    *   **Mechanism:** When the CPU generates a logical address, the MMU *adds* the value from the Relocation Register to it to produce the corresponding physical address.
        *   `Physical Address = Logical Address + Relocation Register Value`
    *   **Ensures Relocation:** This allows a program to be loaded anywhere in physical memory. The logical addresses within the program remain unchanged; only the Relocation Register's value changes, making the program *relocatable* at load time or even runtime.

2.  **Limit Register:**why 
    *   **Purpose:** Stores the *size* (or limit) of the current process's logical address space.
    *   **Mechanism:** Before the MMU performs the addition for relocation, it first checks if the generated `Logical Address` is *less than* the value in the Limit Register (`Logical Address < Limit Register`).
    *   **Ensures Protection:**
        *   If `Logical Address < Limit Register`: The address is valid and within the process's allowed memory space. Translation proceeds.
        *   If `Logical Address >= Limit Register`: The address is *outside* the process's bounds. This indicates an illegal memory access attempt (e.g., trying to access another process's memory or OS memory). The MMU traps to the OS, which usually terminates the offending process.
    *   **Example from notes:**
        *   `RR = 100K`, `Limit = 100` (assuming units like 100KB, and 100 words/units).
        *   If CPU generates `Logical Address = 50K`
            *   Is `50K < 100K`? Yes. (Protection Check)
            *   `Physical Address = 50K + 100K = 150K`.
    *   **Important:** Both the Relocation Register and Limit Register are **privileged registers**, meaning only the Operating System (in kernel mode) can modify their contents. This prevents user processes from bypassing memory protection mechanisms.

### 3.4 Dynamic Partitioning

*   **Concept:** Instead of pre-dividing memory, dynamic partitioning allocates memory to processes **on demand**, creating partitions exactly the size a process needs. When a process terminates, its memory is returned to a pool of free memory, which is then fragmented into holes.
*   **Diagram:** Memory is initially one large free block (minus OS). As processes arrive, holes are carved out. As processes terminate, new holes are created.

*   **Key Characteristics:**

    1.  **No Internal Fragmentation:**
        *   **Explanation:** Since partitions are created exactly to the size requested by the process, there is no wasted space *within* the allocated block.
        *   **Advantage:** More efficient use of memory within a given process.

    2.  **External Fragmentation is Present:**
        *   **Explanation:** This is the main drawback. Over time, as processes are loaded and unloaded, the free memory space becomes fragmented into many small, non-contiguous "holes."
        *   **Example from notes:** You might have 1KB, 2KB, 3KB, 5KB free blocks. Total free is 11KB. But if a 4KB process arrives, it might not fit if no *single* hole is large enough. The free space is non-contiguous.
        *   **Consequence:** As memory becomes increasingly fragmented, it can become impossible to allocate a large block to a new process, even if the total amount of free memory is large enough. This reduces the degree of multiprogramming.

*   **Solution to External Fragmentation: Compaction / Defragmentation**
    *   **Concept:** Compaction (also known as defragmentation) is the process of moving all the occupied memory blocks to one end of the physical memory, and all the free memory blocks (holes) to the other end, thus consolidating all small holes into one large contiguous free block.
    *   **How it works:** Imagine shifting all the puzzle pieces to one side to gather all the empty spaces.
    *   **Advantage:** Eliminates external fragmentation, allowing larger processes to be loaded.
    *   **Disadvantages:**
        *   **Costly:** It involves moving large amounts of data, which requires significant CPU time and I/O operations.
        *   **Overhead:** The system must pause while compaction occurs, impacting performance.
        *   Requires dynamic relocation hardware (like the Relocation Register) because process addresses change.

*   **Advantages of Dynamic Partitioning:**
    1.  **Degree of Multiprogramming is Dynamic (Not Fixed):** Can vary based on available memory and process sizes.
    2.  **No Limitation on Size of the Process:** A process can be as large as the total available contiguous free memory.
    3.  **No Internal Fragmentation:** Optimal use of allocated memory per process.

*   **Disadvantages of Dynamic Partitioning:**
    1.  **Allocation & Deallocation of Memory is Complex:** Finding a suitable hole and managing the free list is more involved.
    2.  **External Fragmentation:** The primary problem, necessitating compaction.

## 4. Data Structures for Dynamic Partitioning

To manage free and allocated memory blocks in dynamic partitioning, the OS needs specific data structures.

### 4.1 Bit-Map for Dynamic Partitioning

*   **Concept:** The main memory is divided into fixed-size "allocation units" (e.g., 4 bytes, 1KB). A bit-map is then maintained, where each bit corresponds to an allocation unit.
    *   If a bit is `0`, the corresponding allocation unit is free.
    *   If a bit is `1`, the corresponding allocation unit is occupied.
*   **Diagram:**
    *   Imagine memory blocks (2MB, 4MB, 4MB, 4MB, 2MB) being pulled back from a process.
    *   Below, there are "Memory allocation units" (e.g., 8 units shown).
    *   A bit array `[000111100]` maps to these units:
        *   `000` (first 3 units) are free (Hole).
        *   `1111` (next 4 units) are occupied (Process).
        *   `00` (last 2 units) are free (Hole).

*   **Allocation Unit Size and Memory Waste:**
    *   If **1 bit** in the bitmap represents **4 bytes** of memory:
        *   This means for every 4 bytes of memory, we use 1 bit (1/8 of a byte) to track it.
        *   The overhead for the bitmap itself is (1/8) / 4 = 1/32 or roughly 3% of memory.
        *   **Problem:** If a process needs 5 bytes, and the allocation unit is 4 bytes, it will be allocated 8 bytes (two 4-byte units). This leads to internal fragmentation if the process size is not a multiple of the allocation unit size.
        *   Your notes state "fraction of memory used by bit map 1/33". This calculation implies 1 bit represents 4 bytes. (1 bit / (4 bytes * 8 bits/byte)) = 1/32 of memory. If it's 1/33, it might mean the total includes the bitmap itself: (1/32) / (1 + 1/32) approx 1/33.

*   **Impact of Allocation Unit Size:**
    *   **Large Allocation Unit:** (e.g., 4 KB)
        *   **Fewer bits required for the bitmap:** Bitmap is smaller, less memory overhead for the bitmap itself.
        *   **Increased space wastage (internal fragmentation):** A process might need 1 KB, but gets 4 KB, wasting 3 KB.
    *   **Small Allocation Unit:** (e.g., 4 bytes)
        *   **More bits required for the bitmap:** Bitmap is larger, more memory overhead for the bitmap.
        *   **Decreased space wastage (internal fragmentation):** A process needing 5 bytes gets 8 bytes, wasting 3 bytes, which is a smaller absolute waste than the previous example.
*   **Conclusion:** The choice of allocation unit size is a trade-off between the memory consumed by the bit-map and the internal fragmentation experienced. It's not widely used for dynamic partitioning as **linked lists** are often more flexible.

### 4.2 Linked List for Dynamic Partitioning

*   **Concept:** Maintain a linked list of free and allocated memory blocks. Each node in the list describes a memory segment (either a process `P` or a hole `H`).
*   **Node Structure:** Each node typically contains:
    *   **Type:** `P` (Process) or `H` (Hole)
    *   **Start Address:** The beginning memory address of the block.
    *   **End Address (or Size):** The ending memory address or the size of the block.
*   **Diagram:**
    *   Memory: OS, then blocks P (0-99), H (100-199), P (200-299), H (300-399).
    *   Linked List: `Head -> (P, 0, 99) -> (H, 100, 199) -> (P, 200, 299) -> (H, 300, 399)`
*   **Advantages of Linked List:**
    1.  **Memory Required is Less than Bit-Map:** Especially if memory is sparsely used, or if allocation units are small (making the bitmap huge). This is empirically found (as your notes state "found experimentally").
    2.  **Maintain List in Increasing Order of Starting Address:** This simplifies management, especially for:
        *   **Merging Adjacent Holes:** When a process frees its memory, the OS checks if the newly freed block is adjacent to an existing free hole. If so, they can be merged into a single, larger hole, reducing external fragmentation. This is much easier with a sorted list.
        *   **Deleting and Inserting:** Efficiently managing the list of blocks.
    3.  **Doubly Linked List is More Beneficial:** Allows for easier traversal in both directions, making merging of adjacent holes even more efficient (checking both preceding and succeeding blocks).

## 5. Memory Allocation Strategies (Placement Algorithms)

When a process requests memory, the OS needs a strategy to decide which free hole to allocate it to. These are called placement algorithms.

### 5.1 First Fit

*   **Concept:** Scan the list of free holes from the beginning (or from where the last scan ended for Next Fit) and allocate the *first hole* that is large enough to satisfy the request.
*   **Behavior:**
    *   Fastest allocation time on average because it doesn't need to search the entire list.
    *   Tends to leave small holes at the beginning of the free list, which might be unusable.
    *   Large holes at the end of memory might remain largely untouched.
*   **Diagram Example:**
    *   Memory: OS, H (0-99), P (100-199), H (200-299), P (300-399).
    *   If `P=50 bytes` requests memory: It will be allocated from `H(0-99)`, leaving `H(50-99)` as a smaller hole.

### 5.2 Next Fit

*   **Concept:** Similar to First Fit, but it starts searching for a suitable hole from the location where the *last allocation* left off, rather than from the beginning of the list.
*   **Behavior:**
    *   Aims to distribute holes more evenly throughout memory, reducing the build-up of small holes at the beginning.
    *   Can potentially lead to more external fragmentation overall because it "wraps around" and breaks up large holes anywhere in memory.
    *   Slightly slower than First Fit on average due to the wrapping.

### 5.3 Best Fit

*   **Concept:** Scan the entire list of free holes and allocate the hole that is the *smallest* among those large enough to satisfy the request.
*   **Behavior:**
    *   Aims to minimize internal fragmentation (by fitting the process into the "tightest" possible hole).
    *   Leaves larger holes intact, which might be good for future large requests.
    *   **Problem:** Tends to create many tiny, unusable holes (splinters) that are too small for future requests, leading to more external fragmentation.
    *   Slowest allocation time because it must search the *entire* list of free holes to find the best fit.
*   **Diagram Example:** `P` requests. `H=20MB`, `H=10MB`, `H=5MB`. If P needs 4MB, it takes from `H=5MB`.

### 5.4 Worst Fit

*   **Concept:** Scan the entire list of free holes and allocate the hole that is the *largest* among those large enough to satisfy the request.
*   **Behavior:**
    *   Aims to leave a large remaining hole after allocation, hoping it will be useful for future requests.
    *   **Problem:** Tends to quickly break up the largest available free blocks, potentially making it harder to find large contiguous blocks later. It can lead to severe external fragmentation.
    *   Slowest allocation time, similar to Best Fit, as it also requires searching the entire list.
*   **Diagram Example:** `P` requests. `H=20MB`, `H=10MB`, `H=5MB`. If P needs 4MB, it takes from `H=20MB`, leaving `H=16MB`.

### 5.5 Quick Fit

*   **Concept:** Maintains separate lists of free blocks for *commonly requested sizes*. For example, one list for 2MB holes, another for 4MB holes, another for 8MB holes.
*   **Behavior:**
    *   **Very Fast Allocation:** When a request comes in, it immediately goes to the list for that size and picks the first available block. No scanning needed.
    *   **Disadvantages:**
        *   **Complex Management:** Managing multiple lists can be more complex.
        *   **Wastage for Uncommon Sizes:** If a request doesn't exactly match a common size, it might lead to more internal fragmentation or require falling back to a general allocation strategy.
        *   Suffers if process sizes are highly variable.

### 5.6 Gate Problems on Allocation Strategies

**Gate Q**: Process requests 300K, 25K, 125K, 50K.
Memory has holes: 50K, 150K, 300K, 350K, 600K.

Let's trace allocations for First Fit (FF) and Best Fit (BF):

**First Fit (FF):** (Scan from left)
1.  **Request 300K:**
    *   Holes: (50K), (150K), **(300K)**, (350K), (600K)
    *   FF picks the **300K** hole.
    *   Remaining holes: 50K, 150K, 350K, 600K.
2.  **Request 25K:**
    *   Holes: **(50K)**, (150K), (350K), (600K)
    *   FF picks the **50K** hole, leaving a 25K remainder.
    *   Remaining holes: 25K (from 50K), 150K, 350K, 600K.
3.  **Request 125K:**
    *   Holes: (25K), **(150K)**, (350K), (600K)
    *   FF picks the **150K** hole, leaving a 25K remainder.
    *   Remaining holes: 25K (from 50K), 25K (from 150K), 350K, 600K.
4.  **Request 50K:**
    *   Holes: (25K), (25K), **(350K)**, (600K)
    *   FF picks the **350K** hole, leaving a 300K remainder.
    *   Remaining holes: 25K, 25K, 300K (from 350K), 600K.
    *   **All requests accommodated by First Fit.**

**Best Fit (BF):** (Scan entire list for smallest large enough hole)
1.  **Request 300K:**
    *   Holes: 50K, 150K, **300K**, 350K, 600K
    *   BF picks the **300K** hole.
    *   Remaining holes: 50K, 150K, 350K, 600K.
2.  **Request 25K:**
    *   Holes: **50K**, 150K, 350K, 600K
    *   BF picks the **50K** hole, leaving 25K remainder.
    *   Remaining holes: 25K (from 50K), 150K, 350K, 600K.
3.  **Request 125K:**
    *   Holes: (25K), **150K**, 350K, 600K
    *   BF picks the **150K** hole, leaving 25K remainder.
    *   Remaining holes: 25K (from 50K), 25K (from 150K), 350K, 600K.
4.  **Request 50K:**
    *   Holes: (25K), (25K), 350K, 600K
    *   **Problem:** No hole is 50K or larger *other than* 350K and 600K. The smallest suitable hole is **350K**.
    *   BF picks the **350K** hole, leaving 300K remainder.
    *   Remaining holes: 25K, 25K, 300K, 600K.
    *   **All requests accommodated by Best Fit.**

The question asks for which options work. Your notes show `FF but not BF` as correct, which is inconsistent with my trace. Let me re-evaluate if there's a specific interpretation.
Ah, the diagram shows the final state for the last request being denied for BF. Let's assume the question implicitly asks if *all* requests can be satisfied.

If the diagrams for the answers illustrate the outcome:
*   (a) Either FF or BF: This implies both work. My trace shows this.
*   (b) FF but not BF: This implies BF fails for some request. The diagram shows a 25K block remaining after a 50K request could not be satisfied.
    *   This would happen if, for example, the 50K hole was split into two 25K holes, and the 50K request couldn't find a contiguous 50K block.
    *   In my trace above, the 50K request for BF takes from 350K. If the smallest two holes (25K, 25K) were all that was left for the 50K request, and they were non-contiguous, then BF would fail.

Let's assume the question implies a slightly different initial memory setup or specific sequence where one algorithm fails.
The *usual* behavior:
*   **First Fit** tends to be faster and often performs better than Best Fit in terms of overall memory utilization for long-running systems because it leaves large blocks at the end of memory untouched.
*   **Best Fit** tends to create many small, unusable holes, which can lead to rapid external fragmentation that requires more compaction.

If the question implies that the 50K hole was the *only* option after some prior allocations, and it got split, then BF could indeed fail where FF (which might have taken a larger hole earlier) succeeds. The diagram for (b) seems to depict this scenario where a 50K request cannot be fulfilled by BF (resulting in a 25K leftover that's not usable for 50K). This means the 50K request failed to find a *single* 50K block using Best Fit, even if it had larger blocks available, suggesting some prior fragmentation or specific constraint.

Given the typical behavior, and assuming the intent of the note's answer:
*   **Answer: (b) FF but not BF** (This implies that Best Fit failed to satisfy all requests in this specific scenario due to excessive fragmentation or the specific hole distribution it created).

**Gate Q55: 1000 KB memory managed using variable partitions, no compaction. Two partitions of 200 KB & 260 KB. Smallest allocation request that could be denied (in Kbytes)?**
*   Total Memory = 1000 KB
*   Occupied = 200 KB + 260 KB = 460 KB
*   Free Memory = 1000 KB - 460 KB = 540 KB
*   This 540 KB free memory is distributed as *multiple holes*. Let these holes be X1, X2, X3...
*   The problem implies there are *other* blocks (not specified, but denoted by 'X' in your notes' solution) and then the 200K and 260K blocks.
*   The solution shows: `X + X + X + 200 + 260 = 1000`
    *   `3X + 460 = 1000`
    *   `3X = 540`
    *   `X = 180`
*   This means the 540KB of free memory is split into *three equal holes of 180KB each*. (This is a specific assumption to make the problem solvable with only 'X' variables).
*   So, the free holes are: `180KB, 180KB, 180KB`.
*   **"Smallest allocation request that could be denied"**: This means the smallest request that *cannot* be satisfied by any of the available holes.
*   The largest single hole available is 180KB.
*   Any request `<= 180KB` can be satisfied.
*   Any request `> 180KB` will be denied.
*   The *smallest* request that would be denied is `181KB`.
*   **Answer: (b) 181** (This solution assumes the free space is fragmented into equal 180KB chunks. If the holes were, say, 50KB, 100KB, 390KB, then 391KB would be denied).

**Gate Q98 (Best-Fit Algorithm and Job Completion):**
This problem requires a step-by-step trace of how jobs are allocated and when they complete.

*   **Partitions:** 4K, 8K, 20K, 2K (ordered by address maybe, not size)
*   **Jobs (size, execution time):**
    *   J1: (3K, 10)
    *   J2: (6K, 2)
    *   J3: (10K, 1)
    *   J4: (20K, 8)
    *   J5: (2K, 6)
    *   J6: (14K, 4)

Let's assume jobs arrive in the order listed and are allocated using **Best Fit**. Best Fit means finding the *smallest* hole that is *large enough*.

**Initial Partitions (Free):**
*   Hole 1: 2K
*   Hole 2: 4K
*   Hole 3: 8K
*   Hole 4: 20K

**Time 0:**
*   **Job J1 (3K):**
    *   Suitable holes: 4K, 8K, 20K
    *   Best Fit (smallest suitable): 4K
    *   Allocate J1 to 4K partition.
    *   Remaining free: (4K-3K=1K) leftover in 4K partition.
    *   Partitions (Free/Occupied): 2K(H), 1K(H, from 4K), 8K(H), 20K(H), 3K(J1)
*   **Job J2 (6K):**
    *   Suitable holes: 8K, 20K
    *   Best Fit: 8K
    *   Allocate J2 to 8K partition.
    *   Remaining free: (8K-6K=2K) leftover in 8K partition.
    *   Partitions: 2K(H), 1K(H), 2K(H, from 8K), 20K(H), 3K(J1), 6K(J2)
*   **Job J3 (10K):**
    *   Suitable holes: 20K
    *   Best Fit: 20K
    *   Allocate J3 to 20K partition.
    *   Remaining free: (20K-10K=10K) leftover in 20K partition.
    *   Partitions: 2K(H), 1K(H)i ko, 2K(H), 10K(H, from 20K), 3K(J1), 6K(J2), 10K(J3)
*   **Job J4 (20K):** No hole of 20K. This job waits.
*   **Job J5 (2K):**
    *   Suitable holes: 2K, 2K, 10K.
    *   Best Fit: 2K (either of the two 2K holes). Let's say the initial 2K hole.
    *   Allocate J5 to 2K partition.
    *   Partitions: 1K(H), 2K(H), 10K(H), 3K(J1), 6K(J2), 10K(J3), 2K(J5)
*   **Job J6 (14K):** No hole of 14K or larger. This job waits.

**Let's track job completion times and available memory:**
*   Jobs running: J1 (completes at 0+10=10), J2 (completes at 0+2=2), J3 (completes at 0+1=1), J5 (completes at 0+6=6).

**Time 1:** J3 completes (10K space free).
*   Free holes: 1K, 2K, 10K (from J3)
*   Pending: J4 (20K), J6 (14K)
*   No pending job fits.

**Time 2:** J2 completes (6K space free).
*   Free holes: 1K, 2K, 10K, 6K (from J2).
*   Pending: J4 (20K), J6 (14K)
*   No pending job fits.

**Time 6:** J5 completes (2K space free).
*   Free holes: 1K, 2K, 10K, 6K, 2K (from J5).
*   Pending: J4 (20K), J6 (14K)
*   No pending job fits.

**Time 10:** J1 completes (3K space free).
*   Free holes: 1K, 2K, 10K, 6K, 2K, 3K (from J1).
*   Now we have: 1K, 2K, 2K, 3K, 6K, 10K. Total free = 24K.
*   Pending: J4 (20K), J6 (14K)

*   **Allocate J4 (20K):**
    *   Suitable hole: None. The largest single hole is 10K. Even though total free is 24K, it's fragmented.
    *   This is a situation where compaction would be needed, but the problem states "no compaction" in Gate Q55, which is variable partition. This problem doesn't state it, but usually, without compaction, Best Fit struggles with external fragmentation.

The notes' solution implies that the jobs run in *parallel* and memory is released dynamically. Let's reinterpret the "when will 20k job complete" by tracing the jobs on a timeline considering memory availability and completion. This type of problem usually implies jobs can queue if memory isn't available and are scheduled when space opens up.

The given solution boxes:
*   Initially: 4K, 8K, 20K, 2K partitions.
*   **Jobs as per solution's allocation:**
    *   3K in (0-2) (presumably the 4K slot, making the 20K hole (0-12))
    *   6K in (0-1) (the 8K slot?)
    *   14K in (0-10) (the 20K slot?)
    *   2K in (0-4) (the 2K slot)
    *   This suggests different *job ID to partition ID* mappings than a strict sequential best-fit.

This specific problem trace (from the image) shows how jobs are mapped to partitions. The 20K job (J4) is waiting until a 20K partition becomes free.
*   Initial: (4K, 8K, 20K, 2K)
*   J1 (3K) -> 4K (leaving 1K)
*   J2 (6K) -> 8K (leaving 2K)
*   J3 (10K) -> 20K (leaving 10K)
*   J5 (2K) -> 2K (leaving 0K)

Now, the "20K Job" (J4) needs 20K. No *single* 20K partition is left. The 20K partition previously held 10K, now it has 10K remaining. J3 just completed from this 20K slot, so 10K + 10K (leftover) = 20K.
So J4 would occupy the 20K partition *after* J3 finishes.
*   J3 time for execution: 1 unit.
*   J3 starts at time 0, finishes at time 1.
*   At time 1, the 20K partition becomes fully available.
*   J4 (20K job) needs 8 units for execution.
*   J4 starts at time 1.
*   J4 completes at time 1 + 8 = **9**.

The diagram shows:
*   4K holds 3K (0-2) - (This must mean J1)
*   8K holds 6K (0-1) - (This must mean J2)
*   20K holds 14K (0-10) - (This must mean J6)
*   2K holds 2K (0-4) - (This must mean J5)
*   10K (0-1) - (This must mean J3)
*   20K (11-19) - This means the 20K job (J4) occupies the 20K partition starting from slot 11, finishing at 19. If the starting time is 0, this means it started at 11 and took 8 units, finishing at 19.

This is a very specific scheduling trace. Let's assume the times given (0-2, 0-1, etc.) refer to *intervals* within which they run, and the overall job sequence/time is:

**Initial State:** Partitions: P1(4K), P2(8K), P3(20K), P4(2K). Jobs: J1(3K, 10), J2(6K, 2), J3(10K, 1), J4(20K, 8), J5(2K, 6), J6(14K, 4).

**Allocation based on the solution diagram:**
*   **J1 (3K)** assigned to P1 (4K). Runs for 10. `Finishes @ 10`. (P1 now has 1K hole)
*   **J2 (6K)** assigned to P2 (8K). Runs for 2. `Finishes @ 2`. (P2 now has 2K hole)
*   **J3 (10K)** assigned to a 10K hole. Let's say this implies it took the 20K partition, leaving 10K. Runs for 1. `Finishes @ 1`. (P3 now has 10K hole + 10K free, total 20K free)
*   **J5 (2K)** assigned to P4 (2K). Runs for 6. `Finishes @ 6`. (P4 now free)

**At Time 0:** J1, J2, J3, J5 start running.
**At Time 1:** J3 (10K, 1) completes. The 20K partition (P3) is now entirely free (initially 20K, J3 took 10K, left 10K, now J3 frees its 10K, so 10K + 10K = 20K free).
**At Time 2:** J2 (6K, 2) completes. 8K partition (P2) is free.
**At Time 6:** J5 (2K, 6) completes. 2K partition (P4) is free.
**At Time 10:** J1 (3K, 10) completes. 4K partition (P1) is free.

**Question: When will the 20K job (J4) complete?**
J4 needs 20K.
*   At Time 0: J4 cannot start, 20K partition is taken by J3.
*   At Time 1: J3 completes, releasing the 10K it used within the 20K partition. The 20K partition becomes entirely free.
*   **J4 (20K, 8) can start at Time 1.**
*   It will run for 8 time units.
*   Therefore, J4 completes at `Start Time + Execution Time = 1 + 8 = 9`.

The solution in your notes has 20K (11-19). This means the 20K job started at time 11 and completed at 19. This would imply that the 20K partition was unavailable until time 11. Perhaps J6 (14K) took the 20K partition *first* (as shown in the solution's diagram, "14K (0-10)" being the 20K slot), and after J6 finished at time 10, J4 started at time 11.

Let's re-trace based on the solution's allocation diagram rather than strict Best Fit:
*   Initial holes: 2K, 4K, 8K, 20K
*   **Assumed Allocation by diagram:**
    *   **3K (J1)** allocated to 4K partition. (Completes @ 10)
    *   **6K (J2)** allocated to 8K partition. (Completes @ 2)
    *   **14K (J6)** allocated to 20K partition. (Completes @ 4)
    *   **2K (J5)** allocated to 2K partition. (Completes @ 6)
    *   **10K (J3)** is unallocated yet. J3 needs 10K.
    *   **20K (J4)** is unallocated yet. J4 needs 20K.

Now, let's track the events and J4's start time:
*   **Time 0:** J1, J2, J5, J6 are running.
*   **Time 2:** J2 (6K) completes. 8K partition becomes free.
*   **Time 4:** J6 (14K) completes. **20K partition becomes free.**
    *   At this point, J3 (10K) and J4 (20K) are pending.
    *   The 20K partition is now available.
    *   **J4 (20K, 8) can now start at Time 4.**
    *   J4 will complete at `4 + 8 = 12`.

This is tricky because the solution's diagram "20K (11-19)" implies J4 starts at 11 and ends at 19. This means something held the 20K slot until time 11. Could J3 (10K) have waited and then occupied the 20K slot after J6, finishing at 4+1 = 5 (if 10K takes 1 time unit and 14K takes 4)? No.

Let's follow the simple interpretation of the visual trace:
`4K` (J1, 3K, 0-2) -> means J1 starts at 0, ends at 2 (takes 3K from 4K partition, so 2 time units for 3K job? No, job time is 10).
The `(0-2), (0-1), (0-10), (0-4)` are *not* execution times but *offsets within the partition*. This problem is poorly annotated in the notes.

**Let's use the execution times and the Best Fit algorithm as commonly understood:**
*   Partitions: P1(4K), P2(8K), P3(20K), P4(2K)
*   Jobs (size, time): J1(3K,10), J2(6K,2), J3(10K,1), J4(20K,8), J5(2K,6), J6(14K,4)

**Time 0:**
*   **J1 (3K):** Best Fit to P1 (4K). P1 now (1K free, J1 3K).
*   **J2 (6K):** Best Fit to P2 (8K). P2 now (2K free, J2 6K).
*   **J3 (10K):** Best Fit to P3 (20K). P3 now (10K free, J3 10K).
*   **J5 (2K):** Best Fit to P4 (2K). P4 now (0K free, J5 2K).
*   **J4 (20K) and J6 (14K) are pending.** No single 20K or 14K free hole.

**Job completion times:**
*   J3 finishes at 0 + 1 = 1. Releases 10K from P3. P3 now has (10K + 10K free) = 20K free.
*   J2 finishes at 0 + 2 = 2. Releases 6K from P2. P2 now has (2K + 6K free) = 8K free.
*   J5 finishes at 0 + 6 = 6. Releases 2K from P4. P4 now has 2K free.
*   J1 finishes at 0 + 10 = 10. Releases 3K from P1. P1 now has (1K + 3K free) = 4K free.

**Scheduling pending jobs:**
*   **At Time 1:** P3 (20K) becomes free.
    *   **J4 (20K, 8) can be scheduled!** It's the largest pending, and a perfect fit.
    *   J4 starts at Time 1.
    *   J4 completes at `1 + 8 = 9`.

This is the most logical interpretation assuming the standard Best Fit algorithm and dynamic scheduling. The annotation `20K (11-19)` for J4 in your notes might represent a specific scenario or a different interpretation of time/scheduling not immediately obvious. Based on standard behavior, **J4 completes at 9.**

## 6. Summary on Partitioning

| Feature             | Static/Fixed Partitioning                               | Dynamic Partitioning                                    |
| :------------------ | :------------------------------------------------------ | :------------------------------------------------------ |
| Internal Fragmentation | **High** (If process < partition size)                    | **None** (Process gets exact size)                        |
| External Fragmentation | **High**                                                | **High**                                                |
| Process Size Limit  | Limited to largest partition size                       | No limitation (can be as large as total free memory)    |
| Degree of MP        | Fixed (limited by # of partitions)                      | Dynamic (more flexible)                                 |
| Complexity          | Simple management                                       | Complex allocation/deallocation                         |
| **Solution for EF** | N/A (cannot compact fixed partitions)                   | **Compaction (costly)**                                 |
| Overall             | Easier to implement, but inefficient memory utilization | More efficient memory usage, but complex to manage EF |

**Non-Contiguous Allocation (Paging/Segmentation)** is the ultimate solution to external fragmentation, as it avoids the need for compaction.

## 7. Overlays

**Problem:** What if the **size of the program is greater than the size of the biggest partition (or available physical memory)?** Traditional contiguous memory allocation would prevent such a program from running.

**Solution: Overlays**
*   **Concept:** Overlays allow a program to run even if its total size exceeds the available physical memory. This is achieved by dividing the program into logical sections (overlays) and loading only the sections that are currently needed into memory.
*   **How it works:**
    *   The program is divided into a **root segment** (which is always resident in memory) and **overlay segments**.
    *   Only one overlay segment from a particular "level" needs to be in memory at a time.
    *   When an overlay segment is needed, it is loaded into the memory area previously occupied by another overlay from the same level, hence "overlaying" it.
    *   Common routines/data that are used by multiple overlays or the root are kept in the root segment or a shared memory area.

**Example: 2-Pass Assembler** (Illustrates memory requirement calculation)
*   **Pass 1 requirements:** 70 KB
*   **Pass 2 requirements:** 80 KB
*   **Symbol Table (ST):** 30 KB
*   **Common Routine (CR):** 20 KB
*   **Overlay Driver (OD):** 10 KB (The code that manages loading overlays)

Total memory without overlays = 70 + 80 + 30 + 20 + 10 = 210 KB.
Suppose available memory is only 200KB. This program cannot run.

**Using Overlays:**
*   The **Symbol Table (ST)**, **Common Routine (CR)**, and **Overlay Driver (OD)** are typically part of the root segment, as they are needed by *both* Pass 1 and Pass 2.
*   Only **one pass** will be in use at any time (either Pass 1 or Pass 2).

**Minimum Partition Size Required:**
1.  **Memory needed for Pass 1:**
    `Pass 1 code + ST + CR + OD = 70 KB + 30 KB + 20 KB + 10 KB = 130 KB`
2.  **Memory needed for Pass 2:**
    `Pass 2 code + ST + CR + OD = 80 KB + 30 KB + 20 KB + 10 KB = 140 KB`

Since only one pass is active at any time, the maximum memory required at any given moment is the maximum of the two pass requirements, plus the common/root parts.
**Minimum Required Partition Size = Max(Pass 1 requirement, Pass 2 requirement) = Max(130 KB, 140 KB) = 140 KB.**

This way, a 210KB logical program can run in a 140KB physical partition.

### 7.1 Overlay Tree

An overlay tree graphically represents the dependencies and structure of a program's overlays.
*   **Root:** The part of the program that is always in memory.
*   **Nodes:** Represent overlay segments.
*   **Branches:** Indicate that child nodes are mutually exclusive with their siblings (i.e., only one child from a set of siblings can be loaded at a time).
*   **Finding Minimum Memory:** To calculate the minimum physical memory required to run a program structured with overlays, you need to find the "heaviest path" from the root to any leaf node. This path represents the maximum amount of code that needs to be resident in memory concurrently.

**Gate Q98: Overlay Tree Problem**
*   **Overlay Tree:**
    *   Root: 2KB
    *   Branch 1: A (4KB)
        *   Child of A: D (6KB), E (8KB)
    *   Branch 2: B (6KB)
        *   Child of B: F (2KB)
    *   Branch 3: C (8KB)
        *   Child of C: G (4KB)

**What is the size of the partition (in physical memory) required to load and run this program?**
*   This asks for the maximum memory footprint at any given time.
*   Calculate the size of each "path" from the Root to a leaf node, considering that siblings are mutually exclusive.

1.  **Path 1 (Root -> A -> D):**
    `Root + A + D = 2KB + 4KB + 6KB = 12 KB`
2.  **Path 2 (Root -> A -> E):**
    `Root + A + E = 2KB + 4KB + 8KB = 14 KB`
    *   (Note: D and E are siblings, so if A is loaded, either D *or* E can be loaded, not both.)
3.  **Path 3 (Root -> B -> F):**
    `Root + B + F = 2KB + 6KB + 2KB = 10 KB`
4.  **Path 4 (Root -> C -> G):**
    `Root + C + G = 2KB + 8KB + 4KB = 14 KB`

**Maximum Memory Required = Max(12 KB, 14 KB, 10 KB, 14 KB) = 14 KB.**

*   **Answer: (b) 14KB**

## 8. Paging

Paging is a non-contiguous memory allocation technique that solves the problem of external fragmentation and the need for compaction. It also forms the basis for **Virtual Memory**.

### 8.1 Core Concepts

*   **Non-Contiguous Allocation:** Unlike contiguous allocation where a process occupies one single block, paging allows a process's memory to be scattered throughout physical memory.
*   **Virtual Address Space (Logical Address Space):** A process sees a contiguous block of memory, which is its own private address space. This is the **logical view**.
*   **Physical Memory (Main Memory):** The actual RAM available.
*   **Pages:** The logical address space of a process is divided into fixed-size blocks called **pages**.
*   **Frames (Page Frames):** The physical memory is divided into fixed-size blocks called **frames**.
*   **Key Principle:** The size of a `page` is *always equal* to the size of a `frame`. (E.g., 4KB page, 4KB frame).

### 8.2 Address Translation with MMU

When the CPU generates a logical address, the MMU (Memory Management Unit) translates it into a physical address.

*   **Logical Address (CPU):** Composed of two parts:
    1.  **Page Number (PNO / Page #):** Identifies which page within the process's logical address space the address belongs to.
    2.  **Offset (Page Offset):** Identifies the location *within* that page.
*   **Physical Address (Main Memory):** Composed of two parts:
    1.  **Frame Number (Frame #):** Identifies which physical frame the page is currently loaded into.
    2.  **Offset (Frame Offset):** This is the *same* as the page offset. (The offset within a page is the same as the offset within a frame, as page size = frame size).

**The Translation Process:**

1.  The CPU generates a `logical address (Page#, Offset)`.
2.  The **MMU** uses the `Page Number` as an index into the **Page Table**.
3.  The **Page Table** contains an entry for each page of the process. Each entry stores the `Frame Number` where that logical page is currently loaded in physical memory.
4.  The MMU retrieves the `Frame Number` from the Page Table entry.
5.  The MMU then constructs the `Physical Address` by combining the retrieved `Frame Number` with the original `Offset`.
    *   `Physical Address = (Frame #, Offset)`

**Example (from notes):**
*   Page Number = 1
*   Offset = 10
*   Page Table lookup for Page 1 reveals it's in Frame 5.
*   Physical Address = Frame 5, Offset 10.

### 8.3 Detailed Paging Example (from notes)

*   **Main Memory Size:** 64 B (Bytes) = 2^6 B
*   **Frame Size:** 4 B = 2^2 B
*   **Number of Frames:** `Main Memory Size / Frame Size = 64 B / 4 B = 16 Frames` (Frames 0 to 15)

*   **Process Size (P1):** 16 B = 2^4 B
*   **Page Size:** 4 B (same as Frame Size)
*   **Number of Pages in Process:** `Process Size / Page Size = 16 B / 4 B = 4 Pages` (Pages 0 to 3)

**Mapping (as per notes' diagram):**
*   Page 0 of P1 goes to Frame 12.
*   Page 1 of P1 goes to Frame 2.
*   Page 2 of P1 goes to Frame 7.
*   Page 3 of P1 goes to Frame 14.

**Page Table for Process P1:**
| Page # | Frame # |
| :----- | :------ |
| 0      | 12      |
| 1      | 2       |
| 2      | 7       |
| 3      | 14      |


**Address Translation Example (from notes):**
*   Logical Address `0110` (binary) for P1.
    *   Assuming 4-bit logical address for 16B process. `16B = 2^4 B`, so 4 bits.
    *   Page size = 4B = 2^2 B, so offset needs 2 bits.
    *   Page number needs `4 - 2 = 2` bits.
    *   Logical address `0110` -> `01` (Page #), `10` (Offset)
        *   Page # `01` is decimal 1.
        *   Offset `10` is decimal 2.
    *   Look up Page 1 in Page Table: Frame is 2.
    *   Physical Address: `Frame # (0010) + Offset (10) = 001010` (binary for decimal 10)
        *   This means Page 1 (logical) is in Frame 2 (physical), and within that frame, the offset is 2.
        *   Physical Address = `(Frame 2 * Frame Size) + Offset = (2 * 4) + 2 = 8 + 2 = 10`.

### 8.4 Word and Address Space

*   **Word:** The smallest addressable unit in a computer system. This is typically 1 byte, but can also be 2, 4, or 8 bytes depending on the architecture. Your notes show `address -> 4x10` grid. This might imply 4 words, each 10 units of data, or 10 words, each 4 units of data. The standard is 1 byte = 1 addressable unit.

*   **Physical Address Space (PAS):** The total range of physical addresses that the CPU can generate. This is equal to the size of main memory.
    *   If Main Memory Size = `M` words, then `Physical Address (PA)` requires `log2(M)` bits.
    *   Example: `PAS = 128 MB`, `WS = 4 B` (Word Size = 4 Bytes)
        *   `128 MB = 128 * 1024 * 1024 B = 2^7 * 2^10 * 2^10 B = 2^27 B`
        *   If `WS = 4 B = 2^2 B`, then `128 MB` contains `2^27 B / 2^2 B = 2^25` words.
        *   Therefore, `PA` (physical address) needs `log2(2^25)` = **25 bits**.

*   **Logical Address Space (LAS):** The total range of logical addresses that a process can generate. This is equal to the size of the process (or virtual memory space allocated to it).
    *   If Process Size = `L` words, then `Logical Address (LA)` requires `log2(L)` bits.
    *   Example: `LAS = 256 MB`, `WS = 4 B`
        *   `256 MB = 2^8 * 2^10 * 2^10 B = 2^28 B`
        *   If `WS = 4 B = 2^2 B`, then `256 MB` contains `2^28 B / 2^2 B = 2^26` words.
        *   Therefore, `LA` (logical address) needs `log2(2^26)` = **26 bits**.

### 8.5 Virtual Memory

*   **Concept:** Virtual memory is a technique that allows the execution of processes that are not entirely in main memory. It creates the illusion that a process has a contiguous block of memory, which can be much larger than the physical RAM available.
*   **How it works:** Whenever the size of a process is larger than the size of main memory (or available contiguous main memory), we can put only a *part* of the process in MM and still be able to run it.
*   **Advantages:**
    *   **Runs Larger Programs:** Enables programs larger than physical memory to be executed.
    *   **Increased Multiprogramming:** More processes can run concurrently because each process doesn't need to load its entire address space into RAM. Only the active parts reside in memory, leading to better CPU utilization.
    *   **Simplified Programming:** Programmers don't need to worry about memory limits or overlays.
    *   **Memory Protection:** Paging provides inherent memory protection by isolating processes.

### 8.6 Page Size and Offset Bits

*   **Relationship:** `Page Size = Frame Size = P words`.
*   The `Offset` part of an address indicates the position *within* a page.
*   **Number of bits required for Offset:** `log2(Page Size in bytes)`.
    *   If Page Size is `P` bytes, then `log2(P)` bits are needed for the offset.
    *   Example: `Page Size = 4 KB`, `Word Size = 4 B`
        *   `4 KB = 4 * 1024 B = 2^2 * 2^10 B = 2^12 B`
        *   Number of words per page = `4 KB / 4 B = 1024 words = 2^10 words`.
        *   Number of bits for Offset = `log2(4KB) = log2(2^12) = 12 bits`. (This is the offset in bytes)
        *   If the offset is in *words*, it would be `log2(1024) = 10 bits`. The notes show 10 bits. This means the offset refers to words, not bytes. It's usually bytes.
        *   **Standard convention:** Offset refers to byte address within the page. So for 4KB page, it's 12 bits.

## 9. Page Table

The Page Table is a crucial data structure for address translation in a paging system.

*   **Purpose:** It maps a process's logical pages to physical frames.
*   **Location:** The Page Table itself resides in **main memory**.
*   **Size:** The size of the Page Table depends on:
    *   The **number of pages** in the logical address space.
    *   The **size of each Page Table Entry (PTE)**.

**Calculations & Formulas:**

*   **Physical Address Space (PAS):** Main Memory Size = `M` words. Physical Address (PA) = `log2(M)` bits (let's call it `m` bits).
*   **Logical Address Space (LAS):** Process Size = `L` words. Logical Address (LA) = `log2(L)` bits (let's call it `l` bits).
*   **Page Size (PS):** `P` bytes (or words). Offset needs `log2(P)` bits (let's call it `p` bits).

**Address Structure:**
*   **Logical Address (LA):** `l` bits total.
    *   `LA = (Page Number (l-p bits), Page Offset (p bits))`
*   **Physical Address (PA):** `m` bits total.
    *   `PA = (Frame Number (m-p bits), Frame Offset (p bits))`

**Number of Pages:**
*   `Number of Pages = LAS / PS = L words / P words = 2^(l-p)` pages.

**Number of Frames:**
*   `Number of Frames = PAS / PS = M words / P words = 2^(m-p)` frames.

**Page Table Entries:**
*   The Page Table will contain one entry for each page in the process's logical address space.
*   **Number of entries in Page Table = Number of Pages = 2^(l-p)**.

**Size of Each Page Table Entry (PTE):**
*   Each entry needs to store the `Frame Number`. The Frame Number is `m-p` bits long.
*   Additionally, each PTE contains control bits (Present/Absent, Modified, Referenced, Protection bits, etc.). Let `e` be the total size of an entry in bits or bytes.
*   So, size of PTE = `(m-p) bits + control bits`. (Your notes denote this as `e` bytes or `m-p` bits directly if only frame no. is considered).

**Total Size of Page Table:**
*   `Size of Page Table = Number of Entries * Size of Each Entry`
*   `Size of Page Table = 2^(l-p) * e (bits or bytes)`

**Example (from notes):**
*   `LA = 27 bits` (So, `l=27`)
*   `PA = 20 bits` (So, `m=20`)
*   `Poff (Offset) = 12 bits` (So, `p=12`)

*   **Calculate Page Number bits:** `l - p = 27 - 12 = 15 bits`.
*   **Calculate Frame Number bits:** `m - p = 20 - 12 = 8 bits`.

*   **Number of entries in Page Table:** `2^(l-p) = 2^15` entries.
*   **Size of each Page Table Entry:** Needs to store Frame Number (8 bits) + control bits. If `e` is assumed to be 8 bits (1 byte) just for the Frame number:
*   **Size of Page Table = `2^15 entries * 8 bits/entry = 2^15 * 1 byte = 32 KB`.**

### 9.1 Page Table Entry (PTE) Structure

A Page Table Entry (PTE) is not just the frame number. It's a structured unit containing various flags and control information used by the MMU and OS:

*   **Page Frame Number:** The most important part, indicating the physical frame where the page resides. (This takes `m-p` bits).
*   **Present/Absent (P/A) bit (or Valid/Invalid bit):**
    *   `1 (Present)`: The page is currently in a physical memory frame.
    *   `0 (Absent)`: The page is not in physical memory (it's on disk, usually in swap space). If this bit is 0, accessing the page causes a **page fault**, triggering the OS to load the page.
*   **Modified (M) bit (or Dirty bit):**
    *   `1`: The page has been written to since it was loaded into memory.
    *   `0`: The page has not been modified.
    *   Used by the OS for efficiency when swapping pages out. If a page hasn't been modified, it doesn't need to be written back to disk; it can just be discarded (if a clean copy exists on disk).
*   **Referenced (R) bit (or Accessed bit):**
    *   `1`: The page has been accessed (read or written) recently.
    *   `0`: The page has not been accessed.
    *   Used by page replacement algorithms (e.g., LRU approximation) to track page usage.
*   **Protection bits:** (e.g., Read/Write/Execute)
    *   Control what operations are allowed on the page (e.g., read-only, read-write, execute-only).
*   **Caching Disabled bit:**
    *   Indicates whether caching is allowed for this specific page. Useful for I/O devices where caching might cause consistency issues.

**CPU to Physical Memory Access with Page Table:**

1.  CPU generates `Logical Address (PNO, Offset)`.
2.  MMU accesses a special register (e.g., **PT-Start Base Register**) which holds the physical base address of the current process's page table.
3.  MMU calculates the address of the specific PTE: `PTE Address = PT-Start Base Register + (PNO * Size of PTE)`.
4.  MMU fetches the PTE from main memory.
5.  MMU checks the Present/Absent bit.
    *   If Absent: Page Fault. OS intervenes.
    *   If Present: MMU extracts the `Frame Number` from the PTE.
6.  MMU forms the `Physical Address = (Frame Number, Offset)`.
7.  MMU accesses the data/instruction at this `Physical Address` in Main Memory.

### 9.2 Gate Problem: Page Table Entry

**Gate Q4:** In a virtual memory system, size of virtual address is 32 bits, size of physical address is 30 bits, page size is 4KB, and size of each page table entry is 32-bit. The main memory is byte addressable. Which of the following is the maximum number of bits that can be used for storing protection & other information in each page table entry?

*   **Virtual Address Size (LA):** 32 bits (`l=32`)
*   **Physical Address Size (PA):** 30 bits (`m=30`)
*   **Page Size (PS):** 4 KB
    *   `4 KB = 4 * 1024 B = 2^2 * 2^10 B = 2^12 B`.
    *   **Offset bits (p):** `log2(2^12) = 12 bits`.
*   **Size of each Page Table Entry (PTE):** 32 bits.

**Calculations:**

1.  **Bits for Page Number (PNO):**
    `PNO bits = Virtual Address bits - Offset bits = 32 - 12 = 20 bits`.

2.  **Bits for Frame Number (FNO):**
    `FNO bits = Physical Address bits - Offset bits = 30 - 12 = 18 bits`.
    *   This is the number of bits required to store the physical frame number in the PTE.

3.  **Total bits available in one PTE:** 32 bits.

4.  **Bits remaining for Protection & Other Info:**
    *   The PTE *must* contain the Frame Number. So, 18 bits are used for the Frame Number.
    *   Bits available for other info = `Total PTE size - FNO bits`
    *   `32 bits - 18 bits = 14 bits`.

*   **Answer: (d) 14**

## 10. Multi-Level Paging

**Problem with Single-Level Page Tables:**
If a process has a very large logical address space (e.g., 32-bit or 64-bit systems), the single-level page table can become extremely large.
*   Example: `LAS = 2^32 B (4GB)`, `Page Size = 4 KB (2^12 B)`
    *   Number of pages = `2^32 / 2^12 = 2^20` pages (1 million pages).
    *   If each PTE is 4 Bytes (32 bits, typical), then the **Page Table Size = 2^20 pages * 4 bytes/page = 4 MB**.
    *   **Issue:** A single 4MB page table needs to be stored contiguously in main memory, which itself is a large memory allocation problem and wastes memory if the process only uses a small fraction of its virtual address space (e.g., a process only needs a few MB, but its page table still needs 4MB).

**Solution: Multi-Level Paging (Hierarchical Paging)**
*   **Concept:** Instead of having one giant page table, the page table itself is paged (or segmented). This means the page table is broken down into smaller, manageable chunks.
*   **How it works (2-Level Paging):**
    1.  The `Logical Address` is divided into three parts:
        *   `Outer Page Table Index` (or Page Directory Index)
        *   `Inner Page Table Index` (or Page Table Index)
        *   `Offset`
    2.  The `Outer Page Table` (or Page Directory) contains entries that point to different `Inner Page Tables`.
    3.  Only the `Outer Page Table` and the `Inner Page Table(s)` corresponding to the currently active parts of the virtual address space need to be in physical memory. Unused portions of the inner page tables can reside on disk.

**Address Translation in 2-Level Paging:**

1.  CPU generates `Logical Address (PT1_Index, PT2_Index, Offset)`.
2.  MMU uses the `PT1_Index` to look up an entry in the `Page Directory (PT1)`. This entry gives the physical address of the relevant `Inner Page Table (PT2)`.
3.  MMU then uses the `PT2_Index` to look up an entry in the `Inner Page Table (PT2)`. This entry contains the `Frame Number`.
4.  Finally, MMU combines the `Frame Number` with the `Offset` to get the `Physical Address`.

**Example (from notes):**
*   `LAS = 2^22 B` (Virtual Address = 22 bits)
*   `Page Size = 4 KB = 2^12 B`
*   `Offset bits = 12 bits`

*   **Remaining bits for Page Number = 22 - 12 = 10 bits.**
*   So, a single-level page table would have `2^10 = 1024` entries.
*   If each PTE (Page Table Entry) is 4 Bytes, then the Page Table Size = `1024 * 4 B = 4 KB`.
    *   This example directly leads to a manageable 4KB page table, implying that `2^22 B` is a small enough LAS that a single-level page table fits perfectly into a single frame (4KB). This is often the design choice for smaller virtual address spaces.

**More Complex Example (from notes, with PT1, PT2):**
*   `LAS = 2^32 B` (Virtual Address = 32 bits)
*   `PS = 4 KB = 2^12 B`
*   `Offset = 12 bits`
*   **Page Number bits = 32 - 12 = 20 bits.**
    *   Total pages = `2^20` pages.
    *   If PTE size = 4 Bytes (32 bits).
    *   Single Page Table Size = `2^20 * 4 B = 4 MB`. This is too large to fit in a single 4KB frame.

**Dividing the 4 MB Page Table into Pages:**
*   Since the page table itself is `4MB` and page size is `4KB`.
*   Number of pages needed for the Page Table = `4 MB / 4 KB = (4 * 1024 KB) / 4 KB = 1024 pages = 2^10 pages`.
*   Each of these 1024 pages of the Page Table will be a `Page Table (PT2)`.
*   The `Page Directory (PT1)` will have `1024` entries, each pointing to one of these `PT2` pages.

**Logical Address Breakdown (for 32-bit LA, 4KB page, 4B PTE):**
*   Total LA = 32 bits
*   Offset = 12 bits (for 4KB page)
*   Remaining for Page Number = 20 bits. These 20 bits are split into two parts:
    *   **PT1 Index:** `log2(Number of PT2 pages) = log2(1024) = 10 bits`.
    *   **PT2 Index:** Remaining bits for PT2 = `20 - 10 = 10 bits`.

*   **Logical Address Format:** `[PT1 Index (10 bits) | PT2 Index (10 bits) | Offset (12 bits)]`

**Advantages of Multi-level Paging:**
*   **Memory Efficiency:** Only the page tables for the active parts of the process are loaded into memory. This saves a lot of physical memory, especially for sparse address spaces.
*   **Reduced Fragmentation:** Further reduces external fragmentation as page tables themselves are now handled in pages.

**Disadvantage:**
*   **Increased Access Time:** A 2-level paging scheme requires *two* memory accesses to translate a logical address to a physical address (one for PT1, one for PT2), plus one more access for the actual data. This is 3 memory accesses per instruction/data access. This overhead is typically mitigated by using a **Translation Lookaside Buffer (TLB)**, which is a fast hardware cache for page table entries.

## 11. Thrashing

*   **Definition:** Thrashing is a phenomenon that occurs in virtual memory systems when a process spends more time paging (swapping pages between main memory and disk) than executing instructions.
*   **Cause:** It typically happens when the degree of multiprogramming is too high, and there isn't enough physical memory to hold the working sets of all running processes. This leads to frequent page faults as processes constantly contend for the limited physical frames.
*   **Symptoms:** Very high page fault rate, very low CPU utilization (as the CPU is mostly idle waiting for disk I/O), and poor system performance.

## 12. Finding Optimal Page Size

Choosing the right page size is a critical design decision in a paging system. It involves a trade-off between various overheads.

**Trade-offs of Page Size:**

*   **Larger Page Size (P â†‘):**
    *   **Pros:**
        *   Fewer pages per process, thus a **smaller page table (PTS â†“)**. Less memory for page tables.
        *   Fewer page faults (if locality is good) because a larger chunk of memory is brought in at once.
        *   More efficient disk I/O (transferring larger blocks is generally faster per byte).
    *   **Cons:**
        *   **Increased Internal Fragmentation:** On average, a process will waste `P/2` bytes at the end of its last allocated page.
        *   Increased startup time for processes (more pages loaded initially).

*   **Smaller Page Size (P â†“):**
    *   **Pros:**
        *   **Reduced Internal Fragmentation.**
        *   Better memory utilization if locality is poor.
    *   **Cons:**
        *   More pages per process, thus a **larger page table (PTS â†‘)**. More memory consumed by page tables.
        *   More frequent page faults (if locality is good, you still only get a small chunk).
        *   Less efficient disk I/O (transferring many small blocks is less efficient).

**Quantifying Overhead:**

The total overhead for a paging system can be modeled as the sum of internal fragmentation and page table memory consumption.

Let:
*   `P` = Page Size (in bytes)
*   `S` = Virtual Address Space size of a process (in bytes)
*   `e` = Size of a Page Table Entry (in bytes)

**Total Overhead = (Average Internal Fragmentation per process) + (Memory consumed by Page Table)**

1.  **Average Internal Fragmentation:**
    *   If a process needs `X` bytes, and page size is `P`, it will be allocated `ceil(X/P) * P` bytes.
    *   The last page might be partially used. On average, the last page is half full.
    *   Average internal fragmentation (waste) per process = `P/2` bytes.

2.  **Memory Consumed by Page Table:**
    *   Number of pages in the process = `S / P`
    *   Memory for page table = `(Number of pages) * (Size of each PTE) = (S/P) * e`

**Total Overhead = (P/2) + (S/P) * e**

**Minimizing the Overhead (Finding Optimal Page Size):**

To find the page size `P` that minimizes this overhead, we can use calculus. We treat Overhead as a function of `P`, differentiate with respect to `P`, and set the derivative to zero.

`Overhead(P) = P/2 + (S*e)/P`

`d(Overhead)/dP = d/dP (P/2) + d/dP ((S*e)*P^-1)`
`d(Overhead)/dP = 1/2 + (S*e) * (-1 * P^-2)`
`d(Overhead)/dP = 1/2 - (S*e) / P^2`

Set derivative to zero to find the minimum:
`1/2 - (S*e) / P^2 = 0`
`1/2 = (S*e) / P^2`
`P^2 = 2 * S * e`
**`P = sqrt(2 * S * e)`**

This formula provides an ideal page size (`P`) that minimizes the total memory overhead considering internal fragmentation and page table size.

**Examples from notes:**

1.  `S = 4 KB = 2^12 B`
    `e = 8 B = 2^3 B`
    `P = sqrt(2 * 2^12 * 2^3) = sqrt(2^1 * 2^12 * 2^3) = sqrt(2^16) = 2^8 = 256 B`

2.  `S = 16 MB = 2^24 B`
    `e = 8 B = 2^3 B`
    `P = sqrt(2 * 2^24 * 2^3) = sqrt(2^28) = 2^14 = 16 KB`

3.  `S = 256 GB = 2^8 * 2^30 B = 2^38 B`
    `e = 32 B = 2^5 B`
    `P = sqrt(2 * 2^38 * 2^5) = sqrt(2^44) = 2^22 = 4 MB`

**Important Note:** While this formula gives a theoretically optimal page size, real-world systems choose page sizes based on other factors too, such as hardware alignment requirements, disk block sizes, and performance implications (TLB hit rates, I/O efficiency). Common page sizes are 4KB, 8KB, 16KB, 64KB, or larger (2MB, 4MB for huge pages).

---

I hope this in-depth explanation, structured into Markdown with detailed examples and clarifications, makes these complex memory management concepts much clearer for you! Please let me know if any part needs further explanation or if you have more questions. I'm here to help you master this topic!